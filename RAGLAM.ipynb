{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Using cached langchain_nomic-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.2.59-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tavily-python\n",
      "  Using cached tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gpt4all\n",
      "  Using cached gpt4all-2.8.2-py3-none-macosx_10_15_universal2.whl.metadata (4.8 kB)\n",
      "Collecting firecrawl-py\n",
      "  Downloading firecrawl_py-1.6.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 (from langchain-nomic)\n",
      "  Using cached langchain_core-0.3.25-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nomic<4.0.0,>=3.1.2 (from langchain-nomic)\n",
      "  Downloading nomic-3.3.4.tar.gz (49 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0.0,>=10.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-nomic) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langsmith<0.3,>=0.1.125 (from langchain_community)\n",
      "  Using cached langsmith-0.2.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub) (24.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (2.5.3)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.20.1-cp312-cp312-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.68.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.2.1-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Using cached orjson-3.10.12-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (13.7.1)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic>=1.9 (from chromadb)\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.45-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from firecrawl-py) (0.21.0)\n",
      "Collecting websockets (from firecrawl-py)\n",
      "  Using cached websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.12/site-packages (from firecrawl-py) (1.6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-nomic) (1.33)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph)\n",
      "  Using cached msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (8.1.7)\n",
      "Collecting jsonlines (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (14.0.2)\n",
      "Requirement already satisfied: pyjwt in /opt/anaconda3/lib/python3.12/site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.8.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.1-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic>=1.9->chromadb)\n",
      "  Using cached pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from chromadb)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-nomic) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Using cached langchain_nomic-0.1.4-py3-none-any.whl (3.9 kB)\n",
      "Using cached langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (982 kB)\n",
      "Using cached langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached chroma_hnswlib-0.7.6-cp312-cp312-macosx_11_0_arm64.whl (185 kB)\n",
      "Using cached langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
      "Downloading langgraph-0.2.59-py3-none-any.whl (135 kB)\n",
      "Using cached tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Using cached gpt4all-2.8.2-py3-none-macosx_10_15_universal2.whl (6.6 MB)\n",
      "Downloading firecrawl_py-1.6.4-py3-none-any.whl (16 kB)\n",
      "Using cached bcrypt-4.2.1-cp39-abi3-macosx_10_12_universal2.whl (489 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Using cached grpcio-1.68.1-cp312-cp312-macosx_10_9_universal2.whl (11.1 MB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached langchain_core-0.3.25-py3-none-any.whl (411 kB)\n",
      "Using cached langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n",
      "Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\n",
      "Downloading langgraph_sdk-0.1.45-py3-none-any.whl (40 kB)\n",
      "Using cached langsmith-0.2.3-py3-none-any.whl (320 kB)\n",
      "Using cached mmh3-5.0.1-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached onnxruntime-1.20.1-cp312-cp312-macosx_13_0_universal2.whl (31.0 MB)\n",
      "Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Using cached orjson-3.10.12-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "Using cached posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached websockets-14.1-cp312-cp312-macosx_11_0_arm64.whl (159 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-macosx_11_0_arm64.whl (104 kB)\n",
      "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl (82 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.1-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached uvloop-0.21.0-cp312-cp312-macosx_10_13_universal2.whl (1.5 MB)\n",
      "Downloading watchfiles-1.0.3-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: nomic\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.3.4-py3-none-any.whl size=49576 sha256=41c2934c584102c4d672bdc157ff1a4d06660dfb019c68bfbedb7c8789bcd53f\n",
      "  Stored in directory: /Users/deveshreemohile/Library/Caches/pip/wheels/71/47/11/51687aff869a2b90e3a6f8c82e93a2ee1cead4cd6fdf604f7f\n",
      "Successfully built nomic\n",
      "Installing collected packages: pypika, monotonic, flatbuffers, durationpy, websockets, uvloop, uvicorn, typing_extensions, types-requests, shellingham, rsa, pyproject_hooks, protobuf, orjson, opentelemetry-util-http, oauthlib, msgpack, mmh3, marshmallow, loguru, jsonlines, importlib-resources, humanfriendly, httpx-sse, httptools, grpcio, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, requests-oauthlib, pydantic-core, posthog, opentelemetry-proto, opentelemetry-api, langchainhub, huggingface-hub, gpt4all, googleapis-common-protos, google-auth, firecrawl-py, coloredlogs, build, typer, tokenizers, tavily-python, pydantic, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, langgraph-sdk, kubernetes, dataclasses-json, pydantic-settings, opentelemetry-sdk, opentelemetry-instrumentation, nomic, langsmith, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, opentelemetry-instrumentation-fastapi, langgraph-checkpoint, langchain-text-splitters, langchain-nomic, langgraph, langchain, chromadb, langchain_community\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: msgpack\n",
      "    Found existing installation: msgpack 1.0.3\n",
      "    Uninstalling msgpack-1.0.3:\n",
      "      Successfully uninstalled msgpack-1.0.3\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.15 durationpy-0.9 fastapi-0.115.6 firecrawl-py-1.6.4 flatbuffers-24.3.25 google-auth-2.37.0 googleapis-common-protos-1.66.0 gpt4all-2.8.2 grpcio-1.68.1 httptools-0.6.4 httpx-sse-0.4.0 huggingface-hub-0.26.5 humanfriendly-10.0 importlib-resources-6.4.5 jsonlines-4.0.0 kubernetes-31.0.0 langchain-0.3.12 langchain-core-0.3.25 langchain-nomic-0.1.4 langchain-text-splitters-0.3.3 langchain_community-0.3.12 langchainhub-0.1.21 langgraph-0.2.59 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.45 langsmith-0.2.3 loguru-0.7.3 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 msgpack-1.1.0 nomic-3.3.4 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 orjson-3.10.12 posthog-3.7.4 protobuf-5.29.1 pydantic-2.10.3 pydantic-core-2.27.1 pydantic-settings-2.7.0 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rsa-4.9 shellingham-1.5.4 starlette-0.41.3 tavily-python-0.5.0 tiktoken-0.8.0 tokenizers-0.20.3 typer-0.15.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 typing_extensions-4.12.2 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT DATA & RETRIEVE DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# URLs list\n",
    "urls = [\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost\",\n",
    "    \"https://www.ai-jason.com/learning-ai/gpt5-llm\",\n",
    "    \"https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3\",\n",
    "]\n",
    "\n",
    "# Load documents using FireCrawlLoader\n",
    "api_key = \"fc-c9d25ee6154640b3831af13a188a1d67\"\n",
    "docs = [FireCrawlLoader(api_key=api_key, url=url, mode=\"scrape\").load() for url in urls]\n",
    "\n",
    "# Flatten the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata and ensure proper document formatting, Metadata Cleaning\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    # Ensure the doc is an instance of Document and has a 'metadata' attribute\n",
    "    if isinstance(doc, Document) and hasattr(doc, \"metadata\"):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Add to vectorDB/store documents in a vector database\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings(), #Embedding model used\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = vectorstore.as_retriever()  #fetching/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Document Content: [Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content=\"[AI Agent](https://www.ai-jason.com/ai/ai-agent)\\n\\n# How to reduce 78%+ of LLM Cost\\n\\n# How to reduce 78%+ of LLM Cost\\n\\nAre you building AI agents or using chatGPT? If so, you may be facing the challenge of high costs associated with large language models (LLM). In this article, we will explore effective strategies to reduce LLM costs by up to 78%. Let's dive in!\\n\\nThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube\\n\\nAI Jason\\n\\n139K subscribers\\n\\n[The REAL cost of LLM (And How to reduce 78%+ of Cost)](https://www.youtube.com/watch?v=lHxl5SchjPA)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nYou're signed out\"), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='Videos you watch may be added to the TV\\'s watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nCancelConfirm\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ ‚Ä¢Live\\n\\n‚Ä¢\\n\\n[Watch on YouTube](https://www.youtube.com/watch?v=lHxl5SchjPA \"Watch on YouTube\")\\n\\n\\u200d\\n\\n## 1\\\\. Change Model\\n\\nOne effective way to reduce LLM costs is to change the model you are using. Different models have different costs associated with them. For example, GPT-4 is the most powerful but also the most expensive model, while Mistro 7B is significantly cheaper. By using a smaller model for specific tasks and reserving the more expensive model for complex questions, you can achieve significant cost savings.'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg)\\n\\n## 2\\\\. Large Language Model Router\\n\\nThe concept of a large language model router involves using a cascade of models to handle different types of questions. Cheaper models are used first, and if they are unable to provide a satisfactory answer, the question is passed on to a more expensive model. This approach leverages the significant cost difference between models and can result in substantial cost savings.\\n\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedae566010ad14fe382cd_llm%20router.jpg)\\n\\n## 3\\\\. Multi-Agent Setup'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='Another strategy is to set up multiple agents, each using a different model. The first agent attempts to complete the task using a cheaper model, and if it fails, the next agent is invoked. By using this multi-agent setup, you can achieve similar or even better success rates while significantly reducing costs.\\n\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedaefa7461fca4f82df51_autogen.jpg)\\n\\n## 4\\\\. LLM Lingua\\n\\nLLM Lingua is a method introduced by Microsoft that focuses on optimizing the input and output of large language models. By removing unnecessary tokens and words from the input, you can significantly reduce the cost of running the model. This method is particularly effective for tasks such as summarization or answering specific questions based on a transcript.\\n\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='## 5\\\\. Optimize Agent Memory\\n\\nOptimizing agent memory is another way to reduce LLM costs. By carefully managing the amount of conversation history stored in memory, you can minimize the number of tokens required for each interaction. This can lead to significant cost savings, especially when dealing with long conversations.\\n\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedb03355b5d49f9ac7f8e_memory%20opt.jpg)\\n\\n## 6\\\\. Observability\\n\\nHaving a deep understanding of the cost patterns in your LLM application is crucial for effective cost optimization. By using observability platforms like L Smith, you can monitor and log the cost for each large language model. This allows you to identify areas where costs can be optimized and make informed decisions to reduce overall expenses.\\n\\nBy implementing these strategies, you can reduce LLM costs by up to 78% or more. Remember, reducing costs while maintaining performance and user experience is a critical skill for AI startups. Stay proactive and continuously optimize your LLM usage to maximize efficiency and profitability.'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='\\u200d\\n\\nGet free HubSpot AI For Marketers Course: [https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\n\\nüîó Links'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='- Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n- Join my AI email list: [https://crafters.ai/](https://crafters.ai/)\\n- My discord: [https://discord.gg/eZXprSaCDE](https://discord.gg/eZXprSaCDE)\\n- Inbox Agent: [https://www.youtube.com/watch?v=Jv\\\\_e6Rt4vWE&t=23s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=Jv_e6Rt4vWE&t=23s&ab_channel=AIJason)\\n- Research Agent: [https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab_channel=AIJason)'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='- James Brigg on Agent Memory: [https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)\\n- Another video about details for LLM cost tracking: [https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab\\\\_channel=LearnfromOpenSourcewithElie](https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab_channel=LearnfromOpenSourcewithElie)'), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content=\"\\u200d\\n\\n## Frequently Asked Questions\\n\\n### Q: How can I determine which model is the most cost-effective for my AI application?\\n\\nA: To determine the most cost-effective model for your AI application, you should consider the specific tasks and requirements of your application. Evaluate the performance and cost trade-offs of different models and choose the one that best fits your needs.\\n\\n### Q: Are there any open-source solutions available for large language model routing?\\n\\nA: While there are no specific open-source solutions for large language model routing, you can explore frameworks like Hugging Face's Hugging GPT, which allows you to build your own routing logic using a large language model as a controller.\\n\\n### Q: How often should I monitor and optimize my LLM costs?\\n\\nA: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\"), Document(metadata={'title': 'How to reduce 78%+ of LLM CostThe REAL cost of LLM (And How to reduce 78%+ of Cost) - YouTube', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'og:title': 'How to reduce 78%+ of LLM Cost', 'og:description': 'AI Agent, Real cost of LLM', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'twitter:title': 'How to reduce 78%+ of LLM Cost', 'twitter:description': 'AI Agent, Real cost of LLM', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'url': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'statusCode': 200}, page_content='A: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\n## Related articles\\n\\n[Browse all articles](https://www.ai-jason.com/)'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content=\"[AI Agent](https://www.ai-jason.com/ai/ai-agent)\\n\\n# GPT5 unlocks LLM System 2 Thinking?\\n\\n# GPT5 unlocks LLM System 2 Thinking?\\n\\nAI has come a long way in recent years, with large language models (LLMs) like GPT-4 impressing us with their ability to generate text. However, these models primarily rely on system one thinking, which is fast and intuitive but lacks the ability to break down complex problems into smaller steps and explore different options. This limitation has led researchers to focus on developing GPT-5 with enhanced reasoning abilities and reliability.\\n\\nGPT5 unlocks LLM System 2 Thinking? - YouTube\\n\\nAI Jason\\n\\n139K subscribers\\n\\n[GPT5 unlocks LLM System 2 Thinking?](https://www.youtube.com/watch?v=sD0X-lWPdxg)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nYou're signed out\"), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='Videos you watch may be added to the TV\\'s watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nCancelConfirm\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ ‚Ä¢Live\\n\\n‚Ä¢\\n\\n[Watch on YouTube](https://www.youtube.com/watch?v=sD0X-lWPdxg \"Watch on YouTube\")\\n\\n## The Two Modes of Thinking\\n\\nIn his book \"Thinking, Fast and Slow,\" Daniel Kahneman introduces the concept of two modes of thinking: system one and system two. System one thinking is our fast, intuitive brain that quickly provides answers based on memorized information. On the other hand, system two thinking is slower but more rational, requiring us to take time, calculate, and analyze before arriving at an answer.'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='Similarly, large language models like GPT-4 primarily rely on system one thinking. They predict the best next words based on the sequence of words they have seen before, without truly understanding the complex problems they are trying to solve.\\n\\n![](https://cdn.prod.website-files.com/img/image-placeholder.svg)\\n\\n\\u200d\\n\\n## The Limitations of GPT-4\\n\\nGPT-4, despite its impressive capabilities, lacks system two thinking. It cannot break down complex tasks into smaller steps or explore different options. It simply generates text based on patterns it has learned from training data. This limitation becomes evident when GPT-4 is faced with complex problems that require deeper analysis and reasoning.\\n\\nFor example, in a video by Veritasium, college students were asked seemingly simple questions like the time it takes for the Earth to go around the Sun. Many of them answered incorrectly because they relied on system one thinking, providing automatic intuitive answers without truly considering the question.'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content=\"Large language models like GPT-4 face a similar challenge. They lack the ability to think critically and break down complex problems into smaller, manageable steps. This is where GPT-5 comes in.\\n\\n## The Promise of GPT-5\\n\\nGPT-5 aims to enhance the reasoning abilities of large language models and introduce system two thinking. OpenAI's Sam Altman mentioned in an interview with Bill Gates that the key milestones for GPT-5 will be around reasoning ability and reliability.\\n\\nCurrently, GPT-4 can reason in extremely limited ways and lacks reliability. It may provide correct answers, but it doesn't always know which answer is the best. GPT-5 aims to improve this by increasing reliability and enhancing reasoning abilities.\\n\\nAltman also mentioned the possibility of GPT-5 being able to solve complex math equations by applying transformations an arbitrary number of times. This would require a more complex control logic for reasoning, going beyond what is currently possible with GPT-4.\\n\\nHowever, simply improving the model itself is not enough. There are ways to enforce system two thinking in large language models today, even with GPT-4.\"), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='## Promoting System 2 Thinking in Large Language Models\\n\\nThere are two common strategies to promote system two thinking in large language models: prompt engineering and communicative agents.\\n\\n### Prompt Engineering\\n\\nPrompt engineering is a simple and common method to guide large language models towards system two thinking. One approach is the \"chain of thought,\" where a sentence is inserted step by step before the model generates any text. This forces the model to break down the problem into smaller steps and think through each one.\\n\\nAnother approach is to provide a few short prompt examples instead of a step-by-step process. These examples guide the model towards thinking through different steps and considering multiple possibilities.\\n\\nWhile prompt engineering can be effective in promoting system two thinking, it has limitations. It often restricts the model to consider only one possibility and may not explore diverse options, similar to how humans approach creative problem-solving.'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content=\"To address this limitation, more advanced prompting tactics like self-consistency with chain of thought (SCCOT) have been proposed. SCCOT involves running the chain of thought process multiple times and reviewing and voting on the most reasonable answers. This allows for some exploration of different options but requires more implementation effort.\\n\\nAnother advanced prompting tactic is the tree of sorts, which simulates a tree search to explore different options and paths. It keeps track of all the paths explored and allows for backtracking if the current path doesn't lead to the desired outcome. However, implementing the tree of sorts is complex and requires significant implementation effort.\\n\\n### Communicative Agents\\n\\nCommunicative agents provide an elegant solution to promote system two thinking in large language models. These are multi-agent setups where users can define different agents and simulate conversations between them. The agents can reflect and spot flaws in each other's perspectives and thinking processes.\\n\\nCommunicative agents have shown promise in enhancing system two thinking. They allow for dedicated agents to review and critique the model's answers, identifying flaws and providing feedback. This collaborative approach mimics how humans solve complex problems by exploring multiple options and learning from each other.\"), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='Setting up communicative agents can be done using various frameworks like ChatGPT, MetaGPT, Autogen, and Crew AI. These frameworks enable the creation of agent workflows and facilitate conversations between agents with different roles, such as problem solvers and reviewers.\\n\\nAutogen Studio, a no-code interface for Autogen, simplifies the setup of communicative agent workflows. It allows for easy collaboration and problem-solving between agents, making it accessible to a wider range of users.\\n\\n## Unlocking System 2 Thinking with GPT-4 Today\\n\\nWhile GPT-4 may not have native system two thinking capabilities, prompt engineering and communicative agents can be used to enforce system two thinking and solve complex tasks.\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides the model towards thinking through problems step by step and considering multiple possibilities. However, prompt engineering may limit exploration and diversity of solutions.'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content=\"Communicative agents, on the other hand, provide a collaborative approach to problem-solving. By simulating conversations between agents, users can leverage the strengths of system one and system two thinking. Reviewers can spot flaws in the model's answers, while problem solvers can iterate and improve their solutions based on feedback.\\n\\nFrameworks like Autogen Studio make it easy to set up communicative agent workflows, allowing for seamless collaboration and problem-solving.\\n\\n## The Future of GPT-5 and System 2 Thinking\\n\\nGPT-5 holds the promise of unlocking system two thinking in large language models. With enhanced reasoning abilities and reliability, GPT-5 aims to bridge the gap between system one and system two thinking, enabling models to solve complex problems more effectively.\\n\\nResearchers are actively working on developing GPT-5 with improved reasoning abilities. The focus is on enabling large language models to break down complex tasks, explore different options, and make more accurate and informed decisions.\\n\\nAs we look forward to the advancements in GPT-5, it's important to continue exploring and implementing strategies like prompt engineering and communicative agents to drive system two thinking in large language models today.\"), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='\\u200d\\n\\nFollow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n\\n\\u200d\\n\\n## FAQs\\n\\n### 1\\\\. Can GPT-4 solve complex problems?\\n\\nGPT-4 can generate text and provide answers, but it primarily relies on system one thinking. It lacks the ability to break down complex problems into smaller steps and explore different options.\\n\\n### 2\\\\. How can prompt engineering promote system two thinking?\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides large language models towards thinking through problems step by step and considering multiple possibilities.\\n\\n### 3\\\\. What are communicative agents?\\n\\nCommunicative agents are multi-agent setups where users can define different agents and simulate conversations between them. This allows for collaborative problem-solving and promotes system two thinking.\\n\\n### 4\\\\. How can communicative agents be set up?\\n\\nFrameworks like Autogen Studio provide a no-code interface for setting up communicative agent workflows. Users can define agents, assign roles, and simulate conversations to solve complex problems.'), Document(metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?GPT5 unlocks LLM System 2 Thinking? - YouTube', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'og:title': 'GPT5 unlocks LLM System 2 Thinking?', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'twitter:title': 'GPT5 unlocks LLM System 2 Thinking?', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'url': 'https://www.ai-jason.com/learning-ai/gpt5-llm', 'statusCode': 200}, page_content='### 5\\\\. What is the future of GPT-5?\\n\\nGPT-5 aims to enhance reasoning abilities and bridge the gap between system one and system two thinking. It holds the promise of enabling large language models to solve complex problems more effectively.\\n\\n## Related articles\\n\\n[Browse all articles](https://www.ai-jason.com/)'), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content='[AI Agent](https://www.ai-jason.com/ai/ai-agent)\\n\\n# Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\n\\n# Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\n\\nAre you looking to build a powerful AI research team? With the latest advancements in AI development, it is now possible to create a group of AI researchers that can work together to extract data, conduct high-quality research, and deliver accurate results. In this step-by-step tutorial, I will show you how to build a multi-agent research system using GPT assistants and Autogen framework. Let\\'s dive in!\\n\\n\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube\\n\\nAI Jason\\n\\n139K subscribers\\n\\n[\"Research agent 3.0 - Build a group of AI researchers\" - Here is how](https://www.youtube.com/watch?v=AVInhYBUnKs)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute'), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content='If playback doesn\\'t begin shortly, try restarting your device.\\n\\nYou\\'re signed out\\n\\nVideos you watch may be added to the TV\\'s watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nCancelConfirm\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ ‚Ä¢Live\\n\\n‚Ä¢\\n\\n[Watch on YouTube](https://www.youtube.com/watch?v=AVInhYBUnKs \"Watch on YouTube\")\\n\\n## Introduction\\n\\nAre you tired of spending hours conducting research and compiling data? Do you wish you had a team of AI researchers that could assist you in extracting information and delivering high-quality research results? Look no further! In this tutorial, I will guide you through the process of building a multi-agent research system using GPT assistants and the Autogen framework. With this system, you can automate your research tasks and improve the quality and efficiency of your work.\\n\\n## Background'), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"Before we dive into the details of building the research system, let's first understand the background and the motivation behind it. Research is a fundamental ability that AI can excel at, and it has a wide range of use cases. Over the past few years, AI development has been rapidly evolving, and new AI researchers with enhanced capabilities are being built regularly. These researchers can perform tasks such as conducting Google searches, browsing the internet, and generating reports based on the collected information.\\n\\n## AI Researcher 2.0\\n\\nThe initial version of the AI researcher, which we will refer to as AI Researcher 2.0, followed a linear process. It was a simple language model chain that could take a research topic as input, trigger a Google search, scrape relevant websites, and generate a report. While this version worked for simple research tasks, it had limitations. For example, if new information was found during the content scraping process, the researcher couldn't further research it. Additionally, it struggled with complex or constrained actions.\\n\\n## Multi-Agent Systems\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"To overcome the limitations of AI Researcher 2.0, multi-agent systems were introduced. These systems, such as MGBT and Chaddef, allowed multiple agents to work together to tackle more complex tasks. The Autogen framework made it easier to create and orchestrate the collaboration between different agents. With the introduction of the Assistant API and GBS by OpenAI, building useful agents became more accessible and cost-effective.\\n\\n## Fine-Tuning\\n\\nWhen training highly specialized agents, there are two common approaches: fine-tuning and knowledge base creation. Fine-tuning is used when you want to improve the model's skills in performing specific tasks, such as data categorization or answering customer emails. Gradient AI is a platform that simplifies the fine-tuning process and makes it accessible to all developers and enterprises. With Gradient AI, you can fine-tune high-performance open-source models without the need for specialized hardware or upfront infrastructure costs.\\n\\n## Building the Research System\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"Now that we have a clear understanding of the background and the tools at our disposal, let's start building the research system. We will create three different GPT assistants with different roles: the user proxy agent, the researcher, and the research manager. Each assistant will play a specific role in the research process, and we will use the Autogen framework to orchestrate their collaboration.\\n\\n## Creating the Research Agents\\n\\nThe first step is to create the research agents. We will start by creating the researcher agent, which will be responsible for browsing the internet and conducting research tasks. The researcher agent will be a GPT assistant with the ability to extract detailed information on any given topic and produce fact-based results. It will perform Google searches, script websites, and provide research references.\\n\\nNext, we will create the research manager agent. The research manager will review the results from the researcher and provide feedback and quality control. The research manager will generate research plans, review the research delivered by the researcher, and propose alternative methods if necessary. The research manager plays a crucial role in ensuring the quality and accuracy of the research results.\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"Finally, we will create the director agent. The director will extract a list of companies to research from an Air table and break it down into individual research tasks. The director will delegate these tasks to the research manager and the researcher. Once a company's research is completed, the director will update the company information in the Air table. The director agent ensures that the research tasks are organized and completed efficiently.\\n\\n## Connecting the Agents\\n\\nNow that we have created the research agents, it's time to connect them together using the Autogen framework. Autogen simplifies the usage of the Assistant API by providing a straightforward way to trigger messages and track progress. We will define the user proxy agent, the researcher agent, and the research manager agent. We will then create a group chat and add the agents to the chat. This will allow them to communicate and collaborate effectively.\\n\\n## Expanding the System\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"One of the most exciting aspects of the research system is its ability to expand and accommodate more agents. You can introduce additional agents, such as a research director, who can break down large research goals into subtasks and delegate them to the research manager and the researcher. The system can also include agents with additional capabilities, such as reading and writing to an Air table to save research results. By expanding the system with more agents, you can enhance its capabilities and make it more autonomous.\\n\\n## Training Specialized Agents\\n\\nTraining specialized agents is crucial for improving the performance of the research system. There are two common approaches to training specialized agents: fine-tuning and knowledge base creation. Fine-tuning is used when you want to improve the model's skills in performing specific tasks. Gradient AI is a platform that simplifies the fine-tuning process and makes it accessible to all developers and enterprises. With Gradient AI, you can fine-tune high-performance open-source models without the need for specialized hardware or upfront infrastructure costs.\\n\\n## Autogen Framework\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"The Autogen framework is a powerful tool for creating and orchestrating multi-agent systems. It provides a flexible and customizable way to define agent roles, trigger messages, and track progress. With Autogen, you can easily create different hierarchies and structures to orchestrate the collaboration between agents. The framework simplifies the usage of the Assistant API and makes it accessible to developers of all skill levels.\\n\\n## Creating the Multi-Agent System\\n\\nNow that we have a clear understanding of the Autogen framework and its capabilities, let's create the multi-agent research system. We will define the user proxy agent, the researcher agent, the research manager agent, and the director agent. We will register the necessary functions for each agent, such as Google search, website scripting, and updating Air table records. By connecting these agents together, we can create a powerful and collaborative research system.\\n\\n## Testing the System\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"Once the multi-agent system is set up, it's time to test its functionality. We can trigger messages to the group chat and observe how the agents collaborate and perform their tasks. We can provide input prompts and evaluate the quality of the research results. By testing the system, we can identify any issues or improvements that need to be made.\\n\\n## Memory Challenges\\n\\nOne of the challenges in building a multi-agent research system is managing memory. As the agents perform research tasks and extract information, they need to remember the information they have found. However, the agents have limited memory capacity, and they can forget information if not managed properly. To overcome this challenge, you can customize the memory allocation for each agent and control the amount of information they can retain.\\n\\n## Customizing the System\\n\\nThe Autogen framework allows you to fully customize the group chat flow and the behavior of the agents. You can define the conversation structure, the prompts, and the actions of each agent. This flexibility enables you to tailor the system to your specific research needs and optimize its performance.\\n\\n## Conclusion\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content='Building a multi-agent research system is a powerful way to automate your research tasks and improve the quality and efficiency of your work. By leveraging GPT assistants and the Autogen framework, you can create a collaborative and autonomous research team. The system allows you to extract data, conduct high-quality research, and deliver accurate results. With the ability to train specialized agents and customize the system, the possibilities are endless. Start building your AI research team today and revolutionize the way you conduct research!\\n\\n\\u200d\\n\\nGithub - Research agents 3.0: [https://www.crafters.ai/aitools/research-agents-3-0](https://www.crafters.ai/aitools/research-agents-3-0)\\n\\nGet free credits to finetune your own LLM on Gradient: [https://gradient.1stcollab.com/aijasonz](https://gradient.1stcollab.com/aijasonz)\\n\\nFollow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n\\n\\u200d\\n\\n\\u200d'), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content=\"## FAQ\\n\\n### Q: Can I expand the research system with more agents?\\n\\nA: Yes, you can expand the research system by introducing more agents with specialized capabilities. This will enhance the system's abilities and make it more autonomous.\\n\\n### Q: How can I train specialized agents?\\n\\nA: You can train specialized agents using fine-tuning or knowledge base creation. Fine-tuning is suitable for improving the model's skills in specific tasks, while knowledge base creation is used for providing accurate and up-to-date information.\\n\\n### Q: Is the Autogen framework easy to use?\\n\\nA: Yes, the Autogen framework simplifies the usage of the Assistant API and provides a straightforward way to define agent roles, trigger messages, and track progress. It is accessible to developers of all skill levels.\\n\\n### Q: How can I manage memory challenges in the research system?\\n\\nA: To manage memory challenges, you can customize the memory allocation for each agent and control the amount of information they can retain. This ensures that the agents remember the necessary information without exceeding their memory capacity.\"), Document(metadata={'title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\"Research agent 3.0 - Build a group of AI researchers\" - Here is how - YouTube', 'description': 'AI Agent,', 'ogTitle': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'ogDescription': 'AI Agent,', 'ogImage': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'ogLocaleAlternate': [], 'og:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'og:description': 'AI Agent,', 'og:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'twitter:title': 'Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial', 'twitter:description': 'AI Agent,', 'twitter:image': 'https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg', 'og:type': 'website', 'twitter:card': 'summary_large_image', 'viewport': ['width=device-width, initial-scale=1', 'width=device-width, initial-scale=1'], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'url': 'https://www.ai-jason.com/learning-ai/how-to-build-ai-agent-tutorial-3', 'statusCode': 200}, page_content='### Q: Can I customize the behavior of the agents in the research system?\\n\\nA: Yes, the Autogen framework allows you to fully customize the behavior of the agents. You can define the conversation structure, the prompts, and the actions of each agent to tailor the system to your specific research needs.\\n\\n## Related articles\\n\\n[Browse all articles](https://www.ai-jason.com/)')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Document Content:\", doc_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the chat model \n",
    "llm = ChatOllama(model=\"llama3:latest\", format=\"json\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "If the document contains keywords related to the user question, grade it as relevant. \n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "\n",
    "Give a binary 'yes' or 'no' score to indicate whether the document is relevant to the question. \n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. \n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the retrieved document: \n",
    "    \n",
    "{document}\n",
    "\n",
    "Here is the user question: {question}\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Initialize the retrieval grader pipeline\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Example inputs\n",
    "question = \"How to reduce LLM COSTS?\"\n",
    "# Assuming `retriever` returns a list of documents\n",
    "docs = retriever.invoke(question)  \n",
    "doc_txt = docs[1].page_content  # Access the page content of the second document\n",
    "\n",
    "# Get the grading result\n",
    "response = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "\n",
    "# Print the result\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs (Large Language Models) provide concise answers based on large datasets, making them useful for quick information retrieval. They can also improve efficiency in tasks such as summarization and translation, allowing for faster processing of complex texts. Additionally, LLMs can generate human-like text and assist with language-related tasks.\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer: \"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM (replace 'local_11m' with the correct model name, e.g., 'llama3:latest')\n",
    "llm = ChatOllama(model=\"llama3:latest\", temperature=0)\n",
    "\n",
    "# Define a function to format documents into a single context string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain the components\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Example usage\n",
    "question = \"What are the benefits of using LLMs?\"\n",
    "retrieved_docs = [\n",
    "    Document(page_content=\"LLMs can provide concise answers based on large datasets.\"),\n",
    "    Document(page_content=\"They can also improve efficiency in tasks like summarization and translation.\"),\n",
    "]\n",
    "\n",
    "# Format the context\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "# Run the chain\n",
    "response = rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TAVILY_API_KEY'] = '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'\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HALLUCINATION GRADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3:latest\", format=\"json\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "Give a  score 'YES' or 'NO' to indicate whether the answer is grounded in / supported by the facts.\n",
    "Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "Example:\n",
    "\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here are the facts:\n",
    "\n",
    "{documents}\n",
    "\n",
    "Here is the answer:\n",
    "\n",
    "{generation}\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "# Create the hallucination grader pipeline\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "# Invoke the grader to assess grounding\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER GRADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|>|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer \n",
    "    is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. < leot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "\\n ----- \\n\n",
    "{generation}\n",
    "\\n ----- \\n\n",
    "Here is the question: {question} <|eot_id|>|start_header_id|>assistant<|end_header_id|‚Ä∫\"\"\",\n",
    " input_variables= [\"generation\", \"question\"],\n",
    ")\n",
    "answer_grader = prompt | llm | JsonOutputParser ()\n",
    "answer_grader. invoke({\"question\": question, \"generation\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LANG GRAPH  - SETUP STATES and NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['END', 'Graph', 'MessageGraph', 'MessagesState', 'START', 'StateGraph', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'add_messages', 'graph', 'message', 'state']\n"
     ]
    }
   ],
   "source": [
    "import langgraph.graph\n",
    "print(dir(langgraph.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x168e981a0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "## STATE\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The question to answer.\n",
    "        generation: The LLM-generated response.\n",
    "        web_search: A flag indicating whether to perform a web search.\n",
    "        documents: A list of retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "## NODES\n",
    "\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Retrieve documents from the vectorstore.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Mock retrieval for demonstration\n",
    "    documents = [\n",
    "        Document(page_content=\"This is document 1.\"),\n",
    "        Document(page_content=\"This is document 2.\"),\n",
    "    ]\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG (Retrieval-Augmented Generation) based on retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with LLM generation.\n",
    "    \"\"\"\n",
    "    print(\"--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Initialize filtered documents and web search flag\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "\n",
    "    # Score each document\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score[\"score\"]\n",
    "\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"--- GRADE: DOCUMENT RELEVANT ---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            # Document not relevant\n",
    "            print(\"--- GRADE: DOCUMENT NOT RELEVANT ---\")\n",
    "            # Set flag to indicate the need for a web search\n",
    "            web_search = \"Yes\"\n",
    "\n",
    "    # Return the updated state\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG (Retrieval-Augmented Generation) based on retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with a new key `generation` that contains the LLM-generated response.\n",
    "    \"\"\"\n",
    "    print(\"--- GENERATE ---\")\n",
    "\n",
    "    # Retrieve the question and documents from the state\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Return the updated state\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    }\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Perform a web search based on the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with web search results appended to the documents.\n",
    "    \"\"\"\n",
    "    print(\"--- WEB SEARCH ---\")\n",
    "\n",
    "    # Extract question and documents from the state\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Perform the web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # Combine the web search results into a single string\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "\n",
    "    # Create a Document object for the web results\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    # Append web results to the existing documents\n",
    "    if documents:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "\n",
    "    # Return the updated state\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or add a web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for the next node to call (\"websearch\" or \"generate\").\n",
    "    \"\"\"\n",
    "    print(\"--- ASSESS GRADED DOCUMENTS ---\")\n",
    "\n",
    "    # Extract required information from the state\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    # Check if web search is required\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered, need to re-generate a new query\n",
    "        print(\"--- DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH ---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # Relevant documents are available, so proceed to generate an answer\n",
    "        print(\"--- DECISION: GENERATE ANSWER BASED ON RELEVANT DOCUMENTS ---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Invoke hallucination grader\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        \n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION VS QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)# web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve \n",
    "workflow.add_node(\"grade_documents\", grade_documents) #grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x168e981a0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Add all conditional edges for `grade_documents` in a single call\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add the rest of the edges\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "\n",
    "# Add conditional edges for `generate`\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RETRIEVE ---\n",
      "'Finisehd running:retrieve:'\n",
      "--CHECK ‚Ä¢ DOCUMENT RELEVANCE TO QUESTION---\n",
      "--GRADE: DOCUMENT RELEVANT---\n",
      "--GRADE: DOCUMENT RELEVANT---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "--- DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH ---\n",
      "'Finisehd running:grade_documents:'\n",
      "--- WEB SEARCH ---\n",
      "'Finisehd running:websearch:'\n",
      "--- GENERATE ---\n",
      "'Finisehd running:generate:'\n",
      "To save on Large Language Model (LLM) costs, optimize your prompts to reduce token usage, use model distillation to transfer knowledge from a large model to a smaller one, and leverage fine-tuning for specialization. Additionally, selecting the right model that balances performance and cost can also help reduce expenses.\n"
     ]
    }
   ],
   "source": [
    "# COMPILE\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "#Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"how to save llm cost?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f'Finisehd running:{key}:')\n",
    "print(value[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
